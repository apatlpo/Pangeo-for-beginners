{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK using HPC cluster through PBS Pro batch queue system.\n",
    "To use pangeo(http://pangeo.io), one need to get used to with some basic python packages used in pangeo.  DASK(http://dask.org) is one of those key component of pangeo.  Through DASK, you can parallelize your computation.  In this notebook, we use dask_jobqueue package based on PBS cluster implementation to show how one can make use. \n",
    "\n",
    "***For MPI users, this is like combination of writing code using mpi_init, submitting mpirun script to a cluster (without knowing what your context of code will be....)***\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Set up python environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Set up dask worker's configuration.  \n",
    "This configuration is based on hal(CNES HPC Cluster), which use PBSPro as batch scheduler.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=6,memory='30 gb', walltime='1:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case you need small chunk of nodes, use following set up.\n",
    "\n",
    "*cluster = PBSCluster(cores=1,memory='5 gb', walltime='1:00:00')*\n",
    "\n",
    "ATT these chunks should be chosen well so that the chunk fits well to the cluster's pbs configuration. In this example it chose 'walltime 1 hour' since that is the max time limit of short quick job queue.\n",
    "\n",
    "**In case you are using HPC cluster, if you make short, and small chunk, your job generally fits to some gap of non used resources of HPC cluster, thus it may start faster.  But this also creates small chunks of used resources, which makes it difficult to run big job for other users.  Thus it is important to consult with your HPC cluster managers.  **\n",
    "\n",
    "---\n",
    "\n",
    "## 3.  Start DASK worker. \n",
    "\n",
    "*** Attention, from this step, you starts to 'occupy' a cluster resource ***\n",
    "***As you 'occupy' a cluster resource, do not forget to kill your 'dask-cluster' using 'qdel' command, or use dask-cluster command 'cluster.close()' from the note book ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = cluster.scale(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Check  your batch job \n",
    "\n",
    "you can do from your terminal following command to see if your pbs jobs are running or not, and if running on which nodes.  You can try to connect those nodes with ssh, and check them with 'top, ps ...' commands to examine how your DASK workers are running. \n",
    "\n",
    "qstat -u your-login -n -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "admin01: \r\n",
      "                                                            Req'd  Req'd   Elap\r\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\r\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\r\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20 node558/0*16\r\n",
      "4410568.admin01 odakat   qt1h     dask-worke   9146   1   6   28gb 01:00 R 00:00 node089/3*6\r\n",
      "4410569.admin01 odakat   qt1h     dask-worke  22172   1   6   28gb 01:00 R 00:00 node098/1*6\r\n",
      "4410570.admin01 odakat   qt1h     dask-worke  22174   1   6   28gb 01:00 R 00:00 node098/2*6\r\n",
      "4410571.admin01 odakat   qt1h     dask-worke  23137   1   6   28gb 01:00 R 00:00 node099/1*6\r\n",
      "4410572.admin01 odakat   qt1h     dask-worke  23152   1   6   28gb 01:00 R 00:00 node099/2*6\r\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00 node104/1*6\r\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00 node107/1*6\r\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00 node114/1*6\r\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00 node115/2*6\r\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00 node137/1*6\r\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat -n -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following command can be used to check how pbs jobs are submitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#!/usr/bin/env bash\n",
      "#PBS -N dask-worker\n",
      "#PBS -l select=1:ncpus=6:mem=28GB\n",
      "#PBS -l walltime=1:00:00\n",
      "JOB_ID=${PBS_JOBID%.*}\n",
      "\n",
      "\n",
      "\n",
      "/home/mp/odakat/miniconda3/envs/equinox/bin/python -m distributed.cli.dask_worker tcp://10.120.43.58:57642 --nthreads 6 --memory-limit 30.00GB --name dask-worker--${JOB_ID}-- --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following commands will enable you to check your DASK client status.  \n",
    "The ***dashboard*** link enable you to monitor your DASK cluster.   \n",
    "- DASK dashbord's worker tab to see how each worker use memory and cpu in a graphical mode.  \n",
    "- 'System' shows system usage of jupyternotebook which host DASK scheduler.  \n",
    "Other tabs are also usefull to understand how parallel process are working.  ATT, it use cpu and memory of your 'jupyter notebook node' if you try to see too complicated graphical interface, your jupyter notebook itself may get slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:57642\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>60</li>\n",
       "  <li><b>Memory: </b>300.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:57642' processes=10 cores=60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Increase  (or scale down) number of workers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale_up(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "admin01: \r\n",
      "                                                            Req'd  Req'd   Elap\r\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\r\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\r\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20\r\n",
      "4410568.admin01 odakat   qt1h     dask-worke   9146   1   6   28gb 01:00 R 00:00\r\n",
      "4410569.admin01 odakat   qt1h     dask-worke  22172   1   6   28gb 01:00 R 00:00\r\n",
      "4410570.admin01 odakat   qt1h     dask-worke  22174   1   6   28gb 01:00 R 00:00\r\n",
      "4410571.admin01 odakat   qt1h     dask-worke  23137   1   6   28gb 01:00 R 00:00\r\n",
      "4410572.admin01 odakat   qt1h     dask-worke  23152   1   6   28gb 01:00 R 00:00\r\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00\r\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00\r\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00\r\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00\r\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00\r\n",
      "4410591.admin01 odakat   qt1h     dask-worke  27124   1   6   28gb 01:00 R 00:00\r\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\r\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat |grep dask-work |wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of dask worker increased; you have 11 of them now. One could use other command like\n",
    "*cluster.scale(11)* instead of *cluster.scale_up(11)*.\n",
    "\n",
    "You can also try to decrease the number of workers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "admin01: \r\n",
      "                                                            Req'd  Req'd   Elap\r\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\r\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\r\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:20\r\n",
      "4410573.admin01 odakat   qt1h     dask-worke   4493   1   6   28gb 01:00 R 00:00\r\n",
      "4410574.admin01 odakat   qt1h     dask-worke  17044   1   6   28gb 01:00 R 00:00\r\n",
      "4410575.admin01 odakat   qt1h     dask-worke  17579   1   6   28gb 01:00 R 00:00\r\n",
      "4410576.admin01 odakat   qt1h     dask-worke   2706   1   6   28gb 01:00 R 00:00\r\n",
      "4410577.admin01 odakat   qt1h     dask-worke  17619   1   6   28gb 01:00 R 00:00\r\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your dask-worker been killed for whatever the reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat -u odakat |grep dask-work |awk '{print \"qdel \" $1 }' >./del-daskworker \n",
    "!chmod +x ./del-daskworker\n",
    "!./del-daskworker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:57642\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:57642' processes=0 cores=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-scale your cluster and get back to your parallel computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "admin01: \r\n",
      "                                                            Req'd  Req'd   Elap\r\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\r\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\r\n",
      "4398868.admin01 odakat   qdev     jupyterhub  72639   1  16   61gb 12:00 R 05:21\r\n",
      "4410601.admin01 odakat   qt1h     dask-worke   9444   1   6   28gb 01:00 R 00:00\r\n",
      "4410602.admin01 odakat   qt1h     dask-worke  22503   1   6   28gb 01:00 R 00:00\r\n",
      "4410603.admin01 odakat   qt1h     dask-worke  22505   1   6   28gb 01:00 R 00:00\r\n",
      "4410604.admin01 odakat   qt1h     dask-worke  23454   1   6   28gb 01:00 R 00:00\r\n",
      "4410605.admin01 odakat   qt1h     dask-worke  23455   1   6   28gb 01:00 R 00:00\r\n",
      "4410606.admin01 odakat   qt1h     dask-worke   4765   1   6   28gb 01:00 R 00:00\r\n",
      "4410607.admin01 odakat   qt1h     dask-worke  17752   1   6   28gb 01:00 R 00:00\r\n",
      "4410608.admin01 odakat   qt1h     dask-worke  18295   1   6   28gb 01:00 R 00:00\r\n"
     ]
    }
   ],
   "source": [
    "!qstat -u odakat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. once you finish your computation, do not forget to stop your dask worker with following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equinox",
   "language": "python",
   "name": "equinox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
