{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this note book, is to make you get used to 'zarr' \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 0. What is zarr? \n",
    "\n",
    "## zarr is a file format for storing big data.  It divides your big data  into 'chunk' sized files and store each of your chunk in a 'zarr' directory.  The 'zarr' directory also stores dimensions, coordinates, (or attributes ), all sorts of meta data.  \n",
    "\n",
    "Long story short for netcdf users:  it cuts your netcdf file in small pieces and store them in a directory, and that make your parallelism possible. So, without using MPI-IO, you can write/read your zarr file in parallel. (where as for netcdf file, even you use hdf5/parallel format, if you do not read/write your file from MPI job, you can not read/write your file in parallel. & Netcdf is not thread safe.\n",
    "\n",
    "( for oceanography numerical model users: If you chose to read/write data as zarr (not in netcdf), for each mpi domain that you compute with a mpi process, it just writes your 'zarr' chunk, that may speed up overall computation time by winning IO access...)\n",
    "\n",
    "\n",
    "*** Take away tips for Lustre users: \n",
    "As zarr is composed of many small files, in case you use lustre filesystem, do not forget to change the striping as 1 for the directory you use, before you starts to store your zarr file.\n",
    " `mkdir dir_for_zarr`\n",
    " `lfs setstripe -c 1 dir_for_zarr `\n",
    "Then, save your zarr file in 'dir_for_zarr' ***\n",
    "\n",
    "\n",
    "Link to zarr documentation https://zarr.readthedocs.io\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1.  Set up your python environment \n",
    "call python environments to use xarray and dask, then create a dask-worker cluster (as explained in notebook *1 DASK, with HPC cluster (PBS Pro)* )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(cores=6,memory='30 gb', walltime='1:00:00')\n",
    "w = cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.120.43.58:59577\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.43.58:8787/status' target='_blank'>http://10.120.43.58:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>60</li>\n",
       "  <li><b>Memory: </b>300.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.120.43.58:59577' processes=10 cores=60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Read a zarr file, as xarray data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='/work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr'\n",
    "ds =xr.open_zarr(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, j: 4320, time: 8785)\n",
      "Coordinates:\n",
      "    dtime    (time) datetime64[ns] dask.array<shape=(8785,), chunksize=(8785,)>\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "    iters    (time) int64 dask.array<shape=(8785,), chunksize=(1,)>\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * time     (time) float64 5.702e+06 5.706e+06 5.71e+06 ... 3.732e+07 3.732e+07\n",
      "Data variables:\n",
      "    SST      (time, face, j, i) float32 dask.array<shape=(8785, 13, 4320, 4320), chunksize=(1, 1, 4320, 4320)>\n",
      "\n",
      " data size: 8525.4 GB\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print('\\n data size: %.1f GB' %(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you see that the zarr file which one just read contains dataset of 'SST' in face,i,j,time ***dimention***.  *dtime* is a time cordinate.  *i* and *j* dimention corresponds almost like longtitute and latitude (XC and YC), *face* corresponds to one of 13 patch of earth surface. \n",
    "\n",
    "---\n",
    "\n",
    " You see 'chunksize=(1, 1, 4320, 4320)'  This is like mpi domain decomposition, that the SST is decomposed into these domain, and when needed,  dask's parallel process (workers) will read some of these decomposed chunks in their memory, or on their disk, and make computation.  In zarr language, the directory of zarr contains n files and each files are made of data of these chunk. \n",
    "\n",
    "i.e. if you use parallel file system, with huge data size, one should have less meta-data access.  Thus you better put enough size of data in each chunk, otherwise you'll just kill the meta-data server.  But, if you want to put your data in the cash of disk to have fast read-wrtite of your data, this chunk size should be smaller than the cash size, so that controller considers that these are the 'cachable' small files. (like GPFS...)  In anycase, before you \n",
    "\n",
    "---\n",
    "# 3. Look into a zarr file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zarr file, SST.zarr, is a directory, and it contains, directories which corresponds to the 'corrdinates' dtime, face, i iters, j, time, and 'SST' the Data variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  dtime  face  i  iters  j  SST  time  .zattrs  .zgroup\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variable directory 'SST' contains 114205 file, because 8785 (time) * 13(face) =114205, and each file corresponds to the data in each 'chunk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114205\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/SST/ |wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.0.0\r\n",
      "0.1.0.0\r\n",
      "0.10.0.0\r\n",
      "0.11.0.0\r\n",
      "0.12.0.0\r\n",
      "0.2.0.0\r\n",
      "0.3.0.0\r\n",
      "0.4.0.0\r\n",
      "0.5.0.0\r\n",
      "0.6.0.0\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/SST/ |head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, 0.1.0.0 contains, SST data for time = 0, face=1, and  i= 0-4319, and j= 0- 4319 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  0  .zarray  .zattrs\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"chunks\": [\r\n",
      "        8785\r\n",
      "    ],\r\n",
      "    \"compressor\": {\r\n",
      "        \"blocksize\": 0,\r\n",
      "        \"clevel\": 5,\r\n",
      "        \"cname\": \"lz4\",\r\n",
      "        \"id\": \"blosc\",\r\n",
      "        \"shuffle\": 1\r\n",
      "    },\r\n",
      "    \"dtype\": \"<i8\",\r\n",
      "    \"fill_value\": null,\r\n",
      "    \"filters\": null,\r\n",
      "    \"order\": \"C\",\r\n",
      "    \"shape\": [\r\n",
      "        8785\r\n",
      "    ],\r\n",
      "    \"zarr_format\": 2\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /work/ALT/swot/swotpub/LLC4320/zarr/SST.zarr/dtime/.zarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look into .zarray file to see the encodings of zarr files.  You can reffer to encoding from following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1, 1, 4320, 4320),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " '_FillValue': nan,\n",
       " 'dtype': dtype('float32'),\n",
       " 'coordinates': 'dtime iters'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.SST.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (8785,),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'units': 'hours since 2011-11-15 00:00:00',\n",
       " 'calendar': 'proleptic_gregorian',\n",
       " 'dtype': dtype('int64')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.dtime.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (1,),\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'dtype': dtype('int64')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.iters.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.  Let's try to create a subset of data we just read, and write to another zarr file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (face: 13, i: 4320, j: 4320, time: 1000)\n",
      "Coordinates:\n",
      "    dtime    (time) datetime64[ns] dask.array<shape=(1000,), chunksize=(1000,)>\n",
      "  * face     (face) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "  * i        (i) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "    iters    (time) int64 dask.array<shape=(1000,), chunksize=(1,)>\n",
      "  * j        (j) int64 0 1 2 3 4 5 6 7 ... 4313 4314 4315 4316 4317 4318 4319\n",
      "  * time     (time) float64 5.702e+06 5.706e+06 5.71e+06 ... 9.295e+06 9.299e+06\n",
      "Data variables:\n",
      "    SST      (time, face, j, i) float32 dask.array<shape=(1000, 13, 4320, 4320), chunksize=(1, 1, 4320, 4320)>\n",
      "\n",
      " data size: 970.4 GB\n"
     ]
    }
   ],
   "source": [
    "dsmille=ds.isel(time=slice(0,1000))\n",
    "print(dsmille)\n",
    "print('\\n data size: %.1f GB' %(dsmille.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 5.43 s, total: 1min 45s\n",
      "Wall time: 10min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2b77370e6da0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dsmille.to_zarr('/work/scratch/odakat/test.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338G\t/work/scratch/odakat/test.zarr\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs /work/scratch/odakat/test.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, even the datasize is 970G, it use compressions (heritated from 'compressor 'encoding from orignal file 'ds' which is , \n",
    "** 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0)**\n",
    "so filesize it self is 338G. \n",
    "\n",
    "---\n",
    "\n",
    "# 5. We can try other compression method here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib', 'zstd']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numcodecs import blosc\n",
    "blosc.list_compressors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3, shuffle=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 8.16 s, total: 3min 30s\n",
      "Wall time: 11min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x2b7741d79630>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dsmille.to_zarr('/work/scratch/odakat/testzarr',  encoding={'SST': {'compressor': compressor}} , mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319G\t/work/scratch/odakat/testzarr\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs /work/scratch/odakat/testzarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Well, looks like new compressing made us win about 20 G of space, but took 1.5 min more..\n",
    " \n",
    " ---\n",
    "# 6. clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /work/scratch/odakat/testzarr\n",
    "!rm -rf /work/scratch/odakat/test.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equinox",
   "language": "python",
   "name": "equinox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
